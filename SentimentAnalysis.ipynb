{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGAif4a4AYlh"
      },
      "outputs": [],
      "source": [
        "channels = ['ndtv', 'indiatoday', 'wion']\n",
        "data = {}\n",
        "\n",
        "for c in (channels):\n",
        "  with open(\"/content/\" + c + \".txt\", \"rb\") as file:\n",
        "    data[c] = [file.read().decode(\"utf-8\") ]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.set_option('max_colwidth',150)\n",
        "data_df = pd.DataFrame.from_dict(data).transpose()\n",
        "data_df.columns = ['transcript']\n",
        "data_df = data_df.sort_index()\n",
        "data_df"
      ],
      "metadata": {
        "id": "-Sy7WJ89AbP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.transcript.loc['ndtv']"
      ],
      "metadata": {
        "id": "1qrHhA2mAe7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "def clean_text_round1(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('\\[.*?\\]', '', text)\n",
        "  text = re.sub('[%s]' % re.escape(string.punctuation), '',text)\n",
        "  text = re.sub('\\w*\\d\\w*', '', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "OpCMiPDHAfy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round1 = lambda x: clean_text_round1(x)\n",
        "data_clean = pd.DataFrame(data_df.transcript.apply(round1))"
      ],
      "metadata": {
        "id": "2DDuyk3GAh2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_round2(text):\n",
        "  text = re.sub('[‘’“”...]', '', text)\n",
        "  text = re.sub('\\n', '', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "wQiEFthMAlkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round2 = lambda x: clean_text_round2(x)\n",
        "data_clean = pd.DataFrame(data_clean.transcript.apply(round2))\n",
        "data_df.to_pickle(\"corpus.pkl\")"
      ],
      "metadata": {
        "id": "vg1itIsTAnR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "data_cv = cv.fit_transform(data_clean.transcript)\n",
        "data_dtm = pd.DataFrame(data_cv.toarray(),\n",
        "columns=cv.get_feature_names())\n",
        "data_dtm.index = data_clean.index\n",
        "data_dtm"
      ],
      "metadata": {
        "id": "okEvn0HOAo2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "data_dtm.to_pickle(\"dtm.pkl\")\n",
        "data_clean.to_pickle('data_clean.pkl')\n",
        "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "jdo4yakJAqbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_pickle('dtm.pkl')\n",
        "data = data.transpose()\n",
        "data.head()"
      ],
      "metadata": {
        "id": "rRzXxsxrAsEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_dict = {}\n",
        "for c in data.columns:\n",
        "  top = data[c].sort_values(ascending=False).head(30)\n",
        "  top_dict[c]= list(zip(top.index, top.values))\n",
        "top_dict"
      ],
      "metadata": {
        "id": "9tYFe_r5AtyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for channel, top_words in top_dict.items():\n",
        "  print(channel)\n",
        "  print(', '.join([word for word, count in top_words[0:20]]))\n",
        "  print('---')"
      ],
      "metadata": {
        "id": "IyC1gqbuA0KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "Yu9JIKNSA2Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_stop_words=stop_words\n",
        "add_stop_words"
      ],
      "metadata": {
        "id": "E7FvrXO2A4m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "putmUDHgA_YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ata_clean = pd.read_pickle('data_clean.pkl')\n",
        "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
        "cv = CountVectorizer(stop_words=stop_words)\n",
        "data_cv = cv.fit_transform(data_clean.transcript)\n",
        "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
        "data_stop.index = data_clean.index\n",
        "import pickle\n",
        "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
        "data_stop.to_pickle(\"dtm_stop.pkl\")"
      ],
      "metadata": {
        "id": "7CXHOdzZBE0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA\n"
      ],
      "metadata": {
        "id": "gu1TcRqNBGv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
        "max_font_size=150, random_state=42)"
      ],
      "metadata": {
        "id": "kU76ue-sBH2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [16, 6]\n",
        "full_names = ['ndtv', 'indiatoday', 'wion']\n",
        "for index, comedian in enumerate(data.columns):\n",
        "  wc.generate(data_clean.transcript[comedian])\n",
        "  plt.subplot(3, 4, index+1)\n",
        "  plt.imshow(wc, interpolation=\"bilinear\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.title(full_names[index])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cjkk-48kBLdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_pickle('corpus.pkl')\n",
        "data"
      ],
      "metadata": {
        "id": "fgKmxwZEBNDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "pol = lambda x: TextBlob(x).sentiment.polarity\n",
        "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
        "data['polarity'] = data['transcript'].apply(pol)\n",
        "data['subjectivity'] = data['transcript'].apply(sub)\n",
        "data"
      ],
      "metadata": {
        "id": "C93f9YsVBO_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis"
      ],
      "metadata": {
        "id": "OK0r4LDtBWZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 8]\n",
        "for index, channel in enumerate(data.index):\n",
        "  x = data.polarity.loc[channel]\n",
        "  y = data.subjectivity.loc[channel]\n",
        "  plt.scatter(x, y, color='blue')\n",
        "  plt.text(x+.001, y+.001, channel, fontsize=10)\n",
        "  plt.xlim(-.01, .30)\n",
        "plt.title('Sentiment Analysis', fontsize=20)\n",
        "plt.xlabel('<-- Negative -------- Positive -->', fontsize=15)\n",
        "plt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sdPzIJLJBQpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "def split_text(text, n=10):\n",
        "\n",
        "    length = len(text)\n",
        "    size = math.floor(length / n)\n",
        "    start = np.arange(0, length, size)\n",
        "    split_list = []\n",
        "    for piece in range(n):\n",
        "      split_list.append(text[start[piece]:start[piece]+size])\n",
        "      return split_list"
      ],
      "metadata": {
        "id": "K4y0XfveBTvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "sTyQxCKSBZFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_pieces = []\n",
        "for t in data.transcript:\n",
        "  split = split_text(t)\n",
        "  list_pieces.append(split)\n",
        "list_pieces"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "n-Zyo84-Bclt",
        "outputId": "a672748e-1e2a-4700-e017-c07623585cb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f36cfd9aab51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlist_pieces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlist_pieces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlist_pieces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic modelling"
      ],
      "metadata": {
        "id": "JycGHrRYBiWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "data = pd.read_pickle('dtm_stop.pkl')\n",
        "data"
      ],
      "metadata": {
        "id": "dWDa78fUBdEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import matutils, models\n",
        "import scipy.sparse"
      ],
      "metadata": {
        "id": "fJrOLlb9BnAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
        "corpus = matutils.Sparse2Corpus(sparse_counts)"
      ],
      "metadata": {
        "id": "OWhacp0uBo5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
        "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
      ],
      "metadata": {
        "id": "Cf35xywVBq7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
        "lda.print_topics()"
      ],
      "metadata": {
        "id": "ILvG-NwRBsft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, passes=10)\n",
        "lda.print_topics()"
      ],
      "metadata": {
        "id": "K7SdfRMKBs1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
        "lda.print_topics()\n"
      ],
      "metadata": {
        "id": "AWWDLHh5BuCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, pos_tag"
      ],
      "metadata": {
        "id": "hnvdNjp3B5qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nouns_adj(text):\n",
        "  '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
        "  is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
        "  tokenized = word_tokenize(text)\n",
        "  nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)]\n",
        "  return ' '.join(nouns_adj)\n"
      ],
      "metadata": {
        "id": "lV68VTeKB8J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean = pd.read_pickle('data_clean.pkl')\n",
        "data_clean"
      ],
      "metadata": {
        "id": "HyXHizbxB9dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "id": "ZGkdypd0B_VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
        "data_nouns_adj"
      ],
      "metadata": {
        "id": "qsquU1rDCAtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "add_stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "B9fSB6-NCCWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new document-term matrix using only nouns and adjectives\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Re-add the additional stop words since we are recreating the document-term matrix\n",
        "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
        "# Recreate a document-term matrix with only nouns and adjectives\n",
        "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
        "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
        "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
        "data_dtmna.index = data_nouns_adj.index\n",
        "data_dtmna"
      ],
      "metadata": {
        "id": "GhKLQrWDCEon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the gensim corpus\n",
        "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n"
      ],
      "metadata": {
        "id": "ytQnBeF3CHds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the vocabulary dictionary\n",
        "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
      ],
      "metadata": {
        "id": "aF86lKsrCJHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start with 2 topics\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna,\n",
        "passes=10)\n",
        "ldana.print_topics()"
      ],
      "metadata": {
        "id": "mDlRlLK9CKv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna,\n",
        "passes=10)\n",
        "ldana.print_topics()\n"
      ],
      "metadata": {
        "id": "yYwgBmu8CMFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna,\n",
        "passes=10)\n",
        "ldana.print_topics()\n"
      ],
      "metadata": {
        "id": "ZS0FfF6MCNoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna,\n",
        "passes=80)\n",
        "ldana.print_topics()"
      ],
      "metadata": {
        "id": "PtvzmvIcCQMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_transformed = ldana[corpusna]\n",
        "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
      ],
      "metadata": {
        "id": "NIv-KdYkCS3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}